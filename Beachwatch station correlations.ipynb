{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beachwatch\n",
    "\n",
    "This notebook examines the bacteria count data for the San Diego coastline, from the Beachwatch program. To analyze beachwatch data, we'll use the data package that is stored on the Library's data repository.\n",
    "\n",
    "First, visit the [repository home page](http://data.sandiegodata.org) and note the tag for \"water-project\" below the search box. The [water-project](https://data.sandiegodata.org/dataset?tags=water-project) tag page lists all of the datasets for this project. In the (San Diego Beachwatch Data)[https://data.sandiegodata.org/dataset/ceden-waterboards-ca-gov-beachwatch-sandiego), Look for these to headings, just above the \"Data and Resources\" section:\n",
    "\n",
    "- Loading the ZIP Package\n",
    "- Loading the CSV Package\n",
    "\n",
    "You can copy the code from one of those sections to get started. \n",
    "\n",
    "After opening the data package, we'll look at the results for the stations and station groups, a examine how well readons at one station are correlate with others in the same group. \n",
    "\n",
    "\n",
    "## Useful sites\n",
    "\n",
    "* A great example of [mapping with geopandas](http://jonathansoma.com/lede/foundations-2017/classes/geopandas/mapping-with-geopandas/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import metapack as mp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Package\n",
    "\n",
    "Usually, the first thing you'll do with a Metatab data package is display the top level documentation, to see what resources it has and other basic information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg = mp.open_package('http://library.metatab.org/ceden.waterboards.ca.gov-beachwatch-sandiego-2.zip')\n",
    "\n",
    "pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a resource gives you the schema. This one isn't complete, since we havent filled in the column descriptions. \n",
    "pkg.resource('beachwatch-sd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the Resource\n",
    "\n",
    "Below is another really common pattern. Get the resoruce and extract a Pandas DataFrame, using read_csv(). (You can also use ``.dataframe()``, which has more accurate datatypes, but is slower. ) We'll do some column modifications immediately, then display the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pkg.resource('beachwatch-sd').read_csv(parse_dates=True)\n",
    "\n",
    "# It looks like the prefix of the station code groups stations, maybe into watersheds. \n",
    "df['stationgroup'] = df.stationcode.str[:2]\n",
    "\n",
    "# The results has a large range, so log transformation makes them easier to visualize.\n",
    "df['log_result'] = df.result.apply(np.log)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['stationcode','stationgroup']].drop_duplicates().groupby('stationgroup').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stationcode'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['analyte'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the following comparisions make sense, we'll want to focus on just one type of bacteria count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.analyte == 'Coliform, Total']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geographic Analysis\n",
    "\n",
    "The Beachwatch data has position information for the stations, which we will need to match the stations to watersheds. Let's start be looking at their positions on a map. Shapely and Geopandas are the main tools for working with geographic data with Pandas and Jupyter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "## Create a new GeoPandas frame, converting the targetlongitude and targetlatitude\n",
    "## colums to a Shapely Point and assigning it to the frame's geometry\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\n",
    "                        [Point(x,y) for x,y in zip(df.targetlongitude, df.targetlatitude)])\n",
    "\n",
    "# Here is a quick plot\n",
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Load a Metapack data package of the US Counties, then extract San Diego county by it's FIPS code, state=6\n",
    "## county=73 \n",
    "counties_pkg = mp.open_package('http://library.metatab.org/census.gov-counties-2017-2.csv')\n",
    "\n",
    "# Use the Metapack feature for turning the Pandas dataframe into a GeoPandas dataframe\n",
    "counties = counties_pkg.resource('counties').geoframe()\n",
    "\n",
    "sd_county = counties[(counties.statefp==6) & (counties.countyfp==73) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the county, then use the same Matplotlib axis to plot the points. \n",
    "base = sd_county.plot(color='white', edgecolor='black', figsize=(8*1.5,8))\n",
    "gdf.plot(ax=base,  column='stationgroup', legend=True)\n",
    "plt.title(\"Beachwatch Program Measurement Locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'EH' group seems really spread out, so let's have a closer look at just that group. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the county, then use the same Matplotlib axis to plot the points. \n",
    "base = sd_county.plot(color='white', edgecolor='black', figsize=(8*1.5,8))\n",
    "gdf[(gdf.stationgroup == 'EH') ].plot(ax=base,  column='stationgroup', legend=True)\n",
    "plt.title(\"Beachwatch Program Measurement Locations for EH group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'EH' group is al over the coast, so the stations in that group probably wont correlate with each otehr very well. Let's exclude it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.stationgroup != 'EH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(18,6))\n",
    "df[(df.stationgroup == 'SE') & (df.sampledate.dt.year == 2004)].groupby('stationcode').plot(ax=ax, x='sampledate', y='result')\n",
    "ax.set_yscale(\"log\", nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(18,6))\n",
    "df[(df.stationcode=='IB-080') & (df.sampledate.dt.year > 2003) & (df.sampledate.dt.year < 2008)].set_index('sampledate').resample('1m').mean().plot(ax=ax, y='result')\n",
    "ax.set_yscale(\"log\", nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,figsize=(18,6))\n",
    "df[(df.stationgroup=='IB') & (df.sampledate.dt.year > 2003) & (df.sampledate.dt.year < 2008)].set_index('sampledate').groupby(['stationname',pd.Grouper(freq='m')])\\\n",
    "    .mean().plot(ax=ax, y='result')\n",
    "ax.set_yscale(\"log\", nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.stationgroup=='IB') & (df.result < 100)].result.hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Station group correlations\n",
    "\n",
    "\n",
    "It is likely that since stations within a group are close to each other, the measures for one group are similar to others in the same group. So, we should try to characterize how well readings between stations in a group are correlated. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = list(df.stationgroup.unique())\n",
    "fig, axes = plt.subplots(len(groups), 1,figsize=(18,5*len(groups)))\n",
    "\n",
    "for ax, group in zip(axes, groups):\n",
    "    \n",
    "    _ = df[(df.stationgroup==group)]\\\n",
    "        .set_index('sampledate').groupby(['stationcode',pd.Grouper(freq='m')]).mean()\n",
    "    _.reset_index().set_index('sampledate').groupby('stationcode').plot(ax=ax,y='result', legend = False)\n",
    "    ax.set_yscale(\"log\", nonposy='clip')\n",
    "    ax.set_title(\"Beachwatch station group {}\".format(group))\n",
    "\n",
    "    \n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "def mean_correlation(df, group, column='result'):\n",
    "    \"\"\"Build the corelation matrix for all columns, remove the diagonal, and average the remaining values. This uses\n",
    "    the full matrix, not the triangular matrix, so each value appears twice. \"\"\"\n",
    "    \n",
    "    _ = df[(df.stationgroup==group)]\\\n",
    "            .set_index('sampledate').groupby(['stationcode',pd.Grouper(freq='m')]).mean()\n",
    "    _ = _.reset_index().set_index(['stationcode','sampledate'])[column].unstack(level=0)\n",
    "    corr = _.corr().stack().to_frame()\n",
    "    corr.columns = [column]\n",
    "    \n",
    "    _ = corr[corr[column] < 1.0]\n",
    "\n",
    "    \n",
    "    return _.mean().iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ (group, mean_correlation(df, group,'result') ) for group in groups ]).sort_values(1, ascending=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([ (group, mean_correlation(df, group,'log_result') ) for group in groups ]).sort_values(1, ascending=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
